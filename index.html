<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <meta name="author" content="Kemal Erdem">

  <title>MSc Thesis Presentation - Kemal Erdem</title>

  <link rel="stylesheet" href="dist/reset.css">
  <link rel="stylesheet" href="dist/reveal.css">
  <link rel="stylesheet" href="dist/custom.css">
  <link rel="stylesheet" href="plugin/pointer/pointer.css">
  <link rel="stylesheet" href="dist/theme/serif.css" id="theme">

  <!-- Theme used for syntax highlighting of code -->
  <link rel="stylesheet" href="css/highlight/isbl-editor-light.css">
</head>
<body>
<div class="reveal">
  <div class="slides">
    <section data-menu-title="Start">
      <span class="heading">Attribution methods in interpretability of CNNs</span><br/>
      <small>Author: Kemal Erdem</small>
    </section>
    <section data-menu-title="Difficult to explain">
      <figure><img src="assets/xai-alien.jpg" style="height: 70vh"><figcaption>Source: <a href="https://twitter.com/mtvcell/status/1287463630078976005">"cell" on Twitter</a></figcaption></figure>
    </section>
    <section data-menu-title="Attribution of what?">
      <span class="heading-secondary">Attribution example</span>
      <figure><img src="assets/spider-man-example.png" style="max-height: 60vh"><figcaption>GradCAM attribution, Source: <a href="https://www.kaggle.com/hchen13/marvel-heroes5">Marvel Heroes Dataset</a></figcaption></figure>
    </section>
    <section data-menu-title="Wrong attribution">
      <span class="heading-secondary">Is that a good attribution?</span>
      <figure><img src="assets/harry-potter.png" style="max-height: 60vh"><figcaption>GradCAM attribution, Source: <a href="https://www.kaggle.com/hchen13/marvel-heroes5">Marvel Heroes Dataset</a></figcaption></figure>
    </section>
    <section data-menu-title="What is a problem">
      <span class="heading-secondary">What is a problem?</span>
      <ul>
        <li>New methods are not validated</li>
        <li>We don't know how to compare XAI methods</li>
        <li>No one is checking the metrics on real data</li>
      </ul>
    </section>
    <section data-menu-title="Qualitative vs Quantitative methods" data-background-image="assets/einstein.jpg">
    </section>
    <section data-menu-title="The Goal">
      <span class="heading">The goal?</span>
        <ul class="fragment">
          <li>Compare metrics (different methods, different models, different datasets)</li>
          <li>Check if metrics make sense (spoiler... they don't)</li>
          <li>Define if we're able to decide which method is better</li>
        </ul>
    </section>
    <section data-menu-title="Different methods different problems">
      <span class="heading-secondary">Different methods different problems</span>
      <figure><img src="assets/methods.png" style="max-height: 60vh"><figcaption>Source: <a href="https://www.kaggle.com/gverzea/edible-wild-plants">Edible plants dataset</a></figcaption></figure>
    </section>

    <section data-menu-title="Maybe metrics then?">
      <span class="heading-secondary">Maybe metrics then?</span>

      <img src="assets/metrics.png">
      <span class="small">Only two metrics available in the most popular XAI library: <a href="https://captum.ai/api/metrics.html">Captum</a></span>
    </section>
    <section data-menu-title="Unintuitive intuition behind the Infidelity">
      <span class="heading-secondary">Unintuitive intuition behind the Infidelity</span>

      <figure style="font-size: 1.8rem">
        $$
        \operatorname{INFD}(\Phi, \mathbf{f}, \mathbf{x})=\mathbb{E}_{\mathbf{I} \sim \mu_{\mathrm{I}}}\left[\left(\mathbf{I}^{T} \Phi(\mathbf{f}, \mathbf{x})-(\mathbf{f}(\mathbf{x})-\mathbf{f}(\mathbf{x}-\mathbf{I}))\right)^{2}\right]
        $$
        $$
        \Phi^{*}(\mathbf{f}, \mathbf{x})=\left(\int \mathbf{I I}^{T} d \mu_{\mathbf{I}}\right)^{-1}\left(\int \mathbf{I I}^{T} I G(\mathbf{f}, \mathbf{x}, \mathbf{I}) d \mu_{\mathbf{I}}\right)
        $$
        $$
        \mathrm{IG}(\mathbf{f}, \mathbf{x}, \mathbf{I})=\int_{t=0}^{1} \nabla \mathbf{f}(\mathbf{x}+(t-1) \mathbf{I})
        $$
        <figcaption style="margin-top: 10px">Infidelity calculation, Source: <a target="_blank" href="https://arxiv.org/pdf/1901.09392.pdf">On the (In)fidelity and Sensitivity of Explanations</a></figcaption>
      </figure>
    </section>
    <section data-menu-title="Actual intuition">
      <span class="heading-secondary">Actual intuition behind the Infidelity</span>
      <figure><img src="assets/inf-sample.png" style="max-height: 60vh"><figcaption>Sample infideilty calculations for different noises.</figcaption></figure>
    </section>
    <section data-menu-title="Datasets" data-background-image="assets/datasets.png" data-background-size="contain">
    </section>
    <section data-menu-title="Experiments">
      <span class="heading-secondary">Experiments</span>
      <div class="display-column medium" style="margin-top: 40px">
        <div>
          <strong>Models</strong>
          <ul>
            <li><a href="https://arxiv.org/abs/1512.03385">ResNet18 (arxiv, 1512.03385)</a></li>
            <li><a href="https://arxiv.org/abs/1905.11946">EfficientNetB0 (arxiv, 1905.11946)</a></li>
            <li><a href="https://arxiv.org/abs/1608.06993">DenseNet121 (arxiv, 1608.06993)</a></li>
            <li>Each with 20%, 40%, 60%, 80% and 100% train data splits <br/> Total of 75 trained models</li>
          </ul>
        </div>
        <div>
          <strong>Methods</strong>
          <ul>
            <li><a href="https://arxiv.org/pdf/1312.6034.pdf">Saliency (arxiv, 1312.6034)</a></li>
            <li><a href="https://arxiv.org/pdf/1311.2901.pdf">Deconvolution (arxiv, 1311.2901)</a></li>
            <li><a href="https://arxiv.org/pdf/1412.6806.pdf">Guided Backpropagation (arxiv, 1412.6806)</a></li>
            <li><a href="https://arxiv.org/pdf/1610.02391.pdf">Guided GradCAM (arxiv, 1610.02391)</a></li>
            <li>Maybe more...</li>
          </ul>
        </div>
      </div>
    </section>
    <section data-menu-title="Work Split">
      <span class="heading-secondary">Work Split</span>
      <p>
        <ul>
      <li><strong>Phase 1</strong>: Publication on Infidelity and Sensitivity as a method to compare XAI solutions (almost done)</li>
          <li><strong>Phase 2</strong>: Master's Thesis which combines Phase 1 and additional work on the XAI methods</li>
        </ul>
      </p>
    </section>
    <section data-menu-title="Bibliography">
      <span class="heading-secondary">Bibliography</span>
      <ul class="small">
        <li><a target="_blank" href="https://arxiv.org/pdf/1901.09392.pdf">On the (In)fidelity and Sensitivity of Explanations, 2019 (arxiv, 1901.09392)</a></li>
        <li><a target="_blank" href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition, 2015 (arxiv, 1512.03385)</a></li>
        <li><a target="_blank" href="https://arxiv.org/abs/1905.11946">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, 2019 (arxiv, 1905.11946)</a></li>
        <li><a target="_blank" href="https://arxiv.org/abs/1608.06993">Densely Connected Convolutional Networks, 2016 (arxiv, 1608.06993)</a></li>
        <li><a target="_blank" href="https://arxiv.org/pdf/1312.6034.pdf">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, 2013 (arxiv, 1312.6034)</a></li>
        <li><a target="_blank" href="https://arxiv.org/pdf/1311.2901.pdf">Visualizing and Understanding Convolutional Networks, 2013 (arxiv, 1311.2901)</a></li>
        <li><a target="_blank" href="https://arxiv.org/pdf/1412.6806.pdf">Striving for Simplicity: The All Convolutional Net, 2014 (arxiv, 1412.6806)</a></li>
        <li><a target="_blank" href="https://arxiv.org/pdf/1610.02391.pdf">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, 2016 (arxiv, 1610.02391)</a></li>
        <li><a target="_blank" href="https://arxiv.org/abs/1810.03292">Sanity Checks for Saliency Maps, 2018 (arxiv, 1810.03292)</a></li>
        <li><a target="_blank" href="https://arxiv.org/abs/1703.01365">Axiomatic Attribution for Deep Networks, 2017 (arxiv, 1703.01365)</a></li>
        <li><a target="_blank" href="https://arxiv.org/abs/1806.10758">A Benchmark for Interpretability Methods in Deep Neural Networks, 2018 (arxiv, 1806.10758)</a></li>
        <li><a target="_blank" href="https://arxiv.org/abs/2003.08754">SAM: The Sensitivity of Attribution Methods to Hyperparameters, 2020 (arxiv, 2003.08754)</a></li>
      </ul>
    </section>
    <section>
      <h2>Thanks</h2>
      <blockquote>"There's no such thing as a stupid question!"</blockquote>
      <p>
        <small>Author: Kemal Erdem</small><br/>
        <small><a href="https://github.com/burnpiro/xai-correlation">GH repo: https://github.com/burnpiro/xai-correlation</a></small><br/>
      </p>
    </section>
  </div>
</div>



<script src="dist/reveal.js"></script>
<script src="plugin/zoom/zoom.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script src="plugin/math/math.js"></script>
<script src="plugin/menu/menu.js"></script>
<script src="plugin/pointer/pointer.js"></script>
<script>
  // More info about config & dependencies:
  // - https://github.com/hakimel/reveal.js#configuration
  // - https://github.com/hakimel/reveal.js#dependencies
  Reveal.initialize({
    hash: true,
    controls: true,
    progress: true,
    slideNumber: true,
    overview: true,
    center: true,
    navigationMode: 'default',
    fragmentInURL: false,
    embedded: false,
    preloadIframes: null,
    autoSlide: 0,
    autoSlideStoppable: true,
    defaultTiming: 120,
    mouseWheel: false,
    previewLinks: false,
    transition: 'slide', // none/fade/slide/convex/concave/zoom
    transitionSpeed: 'default', // default/fast/slow
    backgroundTransition: 'fade', // none/fade/slide/convex/concave/zoom
    display: 'block',
    math: {
      // mathjax: 'plugin/math/MathJax.js', // You can download MathJax locally and keep it in plugins folder
      config: 'TeX-AMS_HTML-full', // See http://docs.mathjax.org/en/latest/config-files.html
      // pass other options into `MathJax.Hub.Config()`
      TeX: { Macros: { RR: "{\\bf R}" } }
    },
    menu: {
      themes: [
        { name: 'Black', theme: 'dist/theme/black.css' },
        { name: 'Moon', theme: 'dist/theme/moon.css' },
        { name: 'Simple', theme: 'dist/theme/simple.css' },
        { name: 'Beige', theme: 'dist/theme/beige.css' },
        { name: 'blood', theme: 'dist/theme/blood.css' },
        { name: 'league', theme: 'dist/theme/league.css' },
        { name: 'night', theme: 'dist/theme/night.css' },
        { name: 'serif', theme: 'dist/theme/serif.css' },
        { name: 'sky', theme: 'dist/theme/sky.css' },
        { name: 'solarized', theme: 'dist/theme/solarized.css' },
        { name: 'white', theme: 'dist/theme/white.css' },
      ]
    },
    pointer: {
      color: "red",
      pointerSize: 18,
      alwaysVisible: false
    },
    plugins: [ RevealZoom, RevealMarkdown, RevealHighlight, RevealNotes, RevealMath, RevealMenu, RevealPointer ]
  });
</script>
</body>
</html>
